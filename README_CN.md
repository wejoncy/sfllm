# SFLLM: é«˜æ€§èƒ½å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ¡†æ¶

[ğŸ‡ºğŸ‡¸ English](./README.md) | [ğŸ‡¨ğŸ‡³ ä¸­æ–‡æ–‡æ¡£](./README_CN.md)

ä¸€ä¸ªç”Ÿäº§å°±ç»ªçš„é«˜æ€§èƒ½å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ¡†æ¶ï¼Œæä¾›ä¸OpenAIå…¼å®¹çš„APIæ¥å£ã€‚

## é¡¹ç›®èƒŒæ™¯

SFLLM (Serving Framework for Large Language Models) æ—¨åœ¨ä¸ºå¤§è¯­è¨€æ¨¡å‹æä¾›é«˜æ•ˆå¯æ‰©å±•çš„æ¨ç†æœåŠ¡ã€‚é¡¹ç›®ä¸“æ³¨äºé€šè¿‡æ™ºèƒ½æ‰¹å¤„ç†ã€CUDAä¼˜åŒ–å’Œå†…å­˜é«˜æ•ˆå®ç°æ¥æœ€å¤§åŒ–GPUåˆ©ç”¨ç‡å¹¶é™ä½æ¨ç†å»¶è¿Ÿã€‚

## åŠŸèƒ½ç‰¹æ€§

- **OpenAIå…¼å®¹API**: å®Œå…¨å…¼å®¹OpenAI APIç«¯ç‚¹ï¼Œå¯ç›´æ¥æ›¿æ¢ä½¿ç”¨
- **é«˜æ€§èƒ½**: é€šè¿‡æ™ºèƒ½è¯·æ±‚æ‰¹å¤„ç†ä¼˜åŒ–æ¨ç†æ€§èƒ½
- **æµå¼æ”¯æŒ**: å®æ—¶æµå¼å“åº”ï¼Œæä¾›æ›´å¥½çš„ç”¨æˆ·ä½“éªŒ
- **CUDAä¼˜åŒ–**: CUDAå›¾å’Œè‡ªå®šä¹‰å†…æ ¸å®ç°æœ€å¤§æ€§èƒ½
- **å†…å­˜é«˜æ•ˆ**: ä¼˜åŒ–çš„KVç¼“å­˜ç®¡ç†å’Œå†…å­˜åˆ†é…
- **ç”Ÿäº§å°±ç»ª**: å†…ç½®å¥åº·æ£€æŸ¥å’Œé”™è¯¯å¤„ç†æœºåˆ¶
- **Eagle3æŠ•æœºè§£ç **: é‡‡ç”¨å…ˆè¿›çš„Eagle3ç®—æ³•è¿›è¡ŒæŠ•æœºè§£ç ï¼Œæ˜¾è‘—æå‡ç”Ÿæˆé€Ÿåº¦
- **é‡å è°ƒåº¦**: æ™ºèƒ½çš„è®¡ç®—ä¸é€šä¿¡é‡å è°ƒåº¦ï¼Œæé«˜æ•´ä½“ååé‡
- **Eagle3 CUDAå›¾åŠ é€Ÿ**: ç»“åˆCUDAå›¾ä¼˜åŒ–çš„Eagle3å®ç°ï¼Œæè‡´æ€§èƒ½è¡¨ç°

## å®‰è£…

### ç¯å¢ƒè¦æ±‚

- Python 3.8+
- CUDA 11.8+ (GPUåŠ é€Ÿéœ€è¦)
- PyTorch 2.0+

### ä»æºç å®‰è£…

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/wejoncy/gemma_serving.git
cd gemma_serving

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# å®‰è£…åŒ…
pip install -e .
```

## å¿«é€Ÿå¼€å§‹

### 1. å¯åŠ¨æœåŠ¡å™¨

**åŸºç¡€ç”¨æ³•ï¼š**
```bash
python python/sfllm/serving/app.py \
  --model /path/to/your/model \
  --port 8081 \
  --dtype float16
```

**å¯ç”¨Eagle3æŠ•æœºè§£ç ï¼š**
```bash
python python/sfllm/serving/app.py \
  --model /path/to/your/model \
  --draft-model-path /path/to/eagle3/draft/model \
  --speculative-algorithm eagle3 \
  --speculative-num-steps 4 \
  --port 8081 \
  --dtype float16
```

### 2. æµ‹è¯•API

**èŠå¤©è¡¥å…¨ï¼ˆæµå¼ï¼‰**
```bash
curl -X POST "http://localhost:8081/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "your-model",
    "messages": [
      {"role": "user", "content": "ä½ å¥½ï¼Œä½ æ€ä¹ˆæ ·ï¼Ÿ"}
    ],
    "stream": true,
    "max_new_tokens": 256,
    "temperature": 0.7
  }'
```

**æ–‡æœ¬è¡¥å…¨**
```bash
curl -X POST "http://localhost:8081/v1/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "your-model",
    "prompt": "äººå·¥æ™ºèƒ½çš„æœªæ¥æ˜¯",
    "max_new_tokens": 128,
    "temperature": 0.8
  }'
```

### 3. å¥åº·æ£€æŸ¥

```bash
curl http://localhost:8081/health
```

## é…ç½®é€‰é¡¹

| é€‰é¡¹ | æè¿° | é»˜è®¤å€¼ |
|------|------|--------|
| `--model` | æ¨¡å‹ç›®å½•è·¯å¾„ | å¿…éœ€ |
| `--port` | æœåŠ¡å™¨ç«¯å£ | 8081 |
| `--dtype` | æ¨¡å‹ç²¾åº¦ (float16/float32) | float16 |
| `--max-context-length` | æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ | 4096 |
| `--cuda-graph-max-bs` | CUDAå›¾æœ€å¤§æ‰¹å¤„ç†å¤§å° | 32 |
| `--disable-cuda-graph` | ç¦ç”¨CUDAå›¾ | False |
| `--speculative-algorithm` | æŠ•æœºè§£ç ç®—æ³• (eagle3) | None |
| `--draft-model-path` | Eagle3è‰ç¨¿æ¨¡å‹è·¯å¾„ | None |
| `--speculative-num-steps` | æŠ•æœºè§£ç æ­¥æ•° | 4 |
| `--disable-overlap` | ç¦ç”¨é‡å è°ƒåº¦ | False |

## å¼€æºè®¸å¯

æœ¬é¡¹ç›®åŸºäºMITè®¸å¯è¯å¼€æº - è¯¦è§ [LICENSE](./LICENSE) æ–‡ä»¶ã€‚

## è´¡çŒ®

æˆ‘ä»¬æ¬¢è¿è´¡çŒ®ï¼è¯·éšæ—¶æäº¤é—®é¢˜å’Œæ‹‰å–è¯·æ±‚ã€‚

---

**Made with â¤ï¸ by [wejoncy](https://github.com/wejoncy)**